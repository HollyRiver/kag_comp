{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8129c6",
   "metadata": {},
   "source": [
    "## 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c89c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc ## for memory\n",
    "import os\n",
    "import copy\n",
    "from math import exp\n",
    "from collections import Counter\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10f2a387-a4e3-4ec4-af72-b5961d718107",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb7e4ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge\n",
      "\n",
      "1 : advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge walk give jump drive bake the sleep night laugh and\n",
      "\n",
      "2 : yuletide decorations gifts cheer holiday carol magi nutcracker polar grinch sleigh chimney workshop stocking ornament holly jingle beard naughty nice\n",
      "\n",
      "3 : yuletide decorations gifts cheer holiday carol magi nutcracker polar grinch sleigh chimney workshop stocking ornament holly jingle beard naughty nice sing cheer and of the is eat visit relax unwrap\n",
      "\n",
      "4 : hohoho candle poinsettia snowglobe peppermint eggnog fruitcake chocolate candy puzzle game doll toy workshop wonder believe dream hope peace joy merry season greeting card wrapping paper bow fireplace night cookie milk star wish wreath angel the to of and in that have it not with as you from we kaggle\n",
      "\n",
      "5 : advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge walk give jump drive bake the sleep night laugh and yuletide decorations gifts cheer holiday carol magi nutcracker polar grinch sleigh chimney workshop stocking ornament holly jingle beard naughty nice sing cheer and of the is eat visit relax unwrap hohoho candle poinsettia snowglobe peppermint eggnog fruitcake chocolate candy puzzle game doll toy workshop wonder believe dream hope peace joy merry season greeting card wrapping paper bow fireplace night cookie milk star wish wreath angel the to of and in that have it not with as you from we kaggle\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(f\"{i} : {df_sample.text[i]}\\n\") for i in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee518372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.50k/1.50k [00:00<00:00, 1.55kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to Santa 2024 - The Perplexity Permutation Puzzle"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.system(\"kaggle competitions submit -c santa-2024 -f sample_submission.csv -m .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097ec0b1",
   "metadata": {},
   "source": [
    "## evaluation 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e10933",
   "metadata": {},
   "source": [
    "`-` 퍼플렉시티 산출을 위해서는 `Gemma 2 9B`모델이 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3840f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## environment variable setting\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "# os.system(\"huggingface-cli login\")\n",
    "PAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\n",
    "DEVICE = torch.device('cuda')\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(\n",
    "    solution: pd.DataFrame,\n",
    "    submission: pd.DataFrame,\n",
    "    row_id_column_name: str,\n",
    "    model_path: str = 'google/gemma-2-9b',\n",
    "    load_in_8bit: bool = False,\n",
    "    clear_mem: bool = False,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates the mean perplexity of submitted text permutations compared to an original text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    solution : DataFrame\n",
    "        DataFrame containing the original text in a column named 'text'.\n",
    "        Includes a row ID column specified by `row_id_column_name`.\n",
    "\n",
    "    submission : DataFrame\n",
    "        DataFrame containing the permuted text in a column named 'text'.\n",
    "        Must have the same row IDs as the solution.\n",
    "        Includes a row ID column specified by `row_id_column_name`.\n",
    "\n",
    "    row_id_column_name : str\n",
    "        Name of the column containing row IDs.\n",
    "        Ensures aligned comparison between solution and submission.\n",
    "\n",
    "    model_path : str, default='/kaggle/input/gemma-2/transformers/gemma-2-9b/2'\n",
    "        Path to the serialized LLM.\n",
    "\n",
    "    load_in_8bit : bool, default=False\n",
    "        Use 8-bit quantization for the model. Requires CUDA.\n",
    "\n",
    "    clear_mem : bool, default=False\n",
    "        Clear GPU memory after scoring by clearing the CUDA cache.\n",
    "        Useful for testing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The mean perplexity score. Lower is better.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ParticipantVisibleError\n",
    "        If the submission format is invalid or submitted strings are not valid permutations.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n",
    "    >>> solution = pd.DataFrame({\n",
    "    ...     'id': [0, 1],\n",
    "    ...     'text': [\"this is a normal english sentence\", \"the quick brown fox jumps over the lazy dog\"]\n",
    "    ... })\n",
    "    >>> submission = pd.DataFrame({\n",
    "    ...     'id': [0, 1],\n",
    "    ...     'text': [\"sentence english normal a is this\", \"lazy the over jumps fox brown quick the dog\"]\n",
    "    ... })\n",
    "    >>> score(solution, submission, 'id', model_path=model_path, clear_mem=True) > 0\n",
    "    True\n",
    "    \"\"\"\n",
    "    # Check that each submitted string is a permutation of the solution string\n",
    "    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n",
    "    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n",
    "    invalid_mask = sol_counts != sub_counts\n",
    "    if invalid_mask.any():\n",
    "        raise ParticipantVisibleError(\n",
    "            'At least one submitted string is not a valid permutation of the solution string.'\n",
    "        )\n",
    "\n",
    "    # Calculate perplexity for the submitted strings\n",
    "    sub_strings = [\n",
    "        ' '.join(s.split()) for s in submission['text'].tolist()\n",
    "    ]  # Split and rejoin to normalize whitespace\n",
    "    scorer = PerplexityCalculator(\n",
    "        model_path=model_path,\n",
    "        load_in_8bit=load_in_8bit,\n",
    "    )  # Initialize the perplexity calculator with a pre-trained model\n",
    "    perplexities = scorer.get_perplexity(\n",
    "        sub_strings\n",
    "    )  # Calculate perplexity for each submitted string\n",
    "\n",
    "    if clear_mem:\n",
    "        # Just move on if it fails. Not essential if we have the score.\n",
    "        try:\n",
    "            scorer.clear_gpu_memory()\n",
    "        except:\n",
    "            print('GPU memory clearing failed.')\n",
    "\n",
    "    return float(np.mean(perplexities))\n",
    "\n",
    "\n",
    "class PerplexityCalculator:\n",
    "    \"\"\"\n",
    "    Calculates perplexity of text using a pre-trained language model.\n",
    "\n",
    "    Adapted from https://github.com/asahi417/lmppl/blob/main/lmppl/ppl_recurrent_lm.py\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_path : str\n",
    "        Path to the pre-trained language model\n",
    "\n",
    "    load_in_8bit : bool, default=False\n",
    "        Use 8-bit quantization for the model. Requires CUDA.\n",
    "\n",
    "    device_map : str, default=\"auto\"\n",
    "        Device mapping for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        load_in_8bit: bool = False,\n",
    "        device_map: str = 'auto',\n",
    "    ):\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n",
    "        # Configure model loading based on quantization setting and device availability\n",
    "        if load_in_8bit:\n",
    "            if DEVICE.type != 'cuda':\n",
    "                raise ValueError('8-bit quantization requires CUDA device')\n",
    "            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n",
    "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=device_map,\n",
    "            )\n",
    "        else:\n",
    "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n",
    "                device_map=device_map,\n",
    "            )\n",
    "\n",
    "        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_perplexity(\n",
    "        self, input_texts: Union[str, List[str]], batch_size: 32\n",
    "    ) -> Union[float, List[float]]:\n",
    "        \"\"\"\n",
    "        Calculates the perplexity of given texts.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_texts : str or list of str\n",
    "            A single string or a list of strings.\n",
    "\n",
    "        batch_size : int, default=None\n",
    "            Batch size for processing. Defaults to the number of input texts.\n",
    "\n",
    "        debug : bool, default=False\n",
    "            Print debugging information.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float or list of float\n",
    "            A single perplexity value if input is a single string,\n",
    "            or a list of perplexity values if input is a list of strings.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> import pandas as pd\n",
    "        >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n",
    "        >>> scorer = PerplexityCalculator(model_path=model_path)\n",
    "\n",
    "        >>> submission = pd.DataFrame({\n",
    "        ...     'id': [0, 1, 2],\n",
    "        ...     'text': [\"this is a normal english sentence\", \"thsi is a slihgtly misspelled zr4g sentense\", \"the quick brown fox jumps over the lazy dog\"]\n",
    "        ... })\n",
    "        >>> perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n",
    "        >>> perplexities[0] < perplexities[1]\n",
    "        True\n",
    "        >>> perplexities[2] < perplexities[0]\n",
    "        True\n",
    "\n",
    "        >>> perplexities = scorer.get_perplexity([\"this is a sentence\", \"another sentence\"])\n",
    "        >>> all(p > 0 for p in perplexities)\n",
    "        True\n",
    "\n",
    "        >>> scorer.clear_gpu_memory()\n",
    "        \"\"\"\n",
    "        single_input = isinstance(input_texts, str)\n",
    "        input_texts = [input_texts] if single_input else input_texts\n",
    "\n",
    "        loss_list = []\n",
    "        \n",
    "        batches = len(input_texts)//batch_size + (len(input_texts)%batch_size != 0)\n",
    "        \n",
    "        for j in range(batches):\n",
    "            a = j*batch_size\n",
    "            b = (j+1)*batch_size\n",
    "            input_batch = input_texts[a:b]\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                # Process each sequence independently\n",
    "                for text in input_texts:\n",
    "                    # Explicitly add sequence boundary tokens to the text\n",
    "                    text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n",
    "\n",
    "                    # Tokenize\n",
    "                    model_inputs = self.tokenizer(\n",
    "                        text_with_special,\n",
    "                        return_tensors='pt',\n",
    "                        add_special_tokens=False,\n",
    "                        padding=True\n",
    "                    )\n",
    "\n",
    "                    if 'token_type_ids' in model_inputs:\n",
    "                        model_inputs.pop('token_type_ids')\n",
    "\n",
    "                    model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n",
    "\n",
    "                    # Get model output\n",
    "                    output = self.model(**model_inputs, use_cache=False)\n",
    "                    logits = output['logits']\n",
    "                    \n",
    "                    label = model_inputs[\"input_ids\"]\n",
    "                    label[label == self.tokenizer.pad_token_type_id] = PAD_TOKEN_LABEL_ID\n",
    "\n",
    "                    # Shift logits and labels for calculating loss\n",
    "                    shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n",
    "                    shift_labels = label[..., 1:].contiguous()  # Drop first input\n",
    "\n",
    "                    # Calculate token-wise loss\n",
    "                    loss = self.loss_fct(\n",
    "                        shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                        shift_labels.view(-1)\n",
    "                    )\n",
    "\n",
    "                    # Calculate average loss\n",
    "                    loss = self.loss_fct(\n",
    "                        shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                        shift_labels.view(-1)\n",
    "                    )\n",
    "\n",
    "                    loss = loss.view(len(logits), -1)\n",
    "                    valid_length = (shift_labels != PAD_TOKEN_LABEL_ID).sum(dim=-1)\n",
    "                    loss = torch.sum(loss, -1) / valid_length\n",
    "\n",
    "                    loss_list += loss.cpu().tolist()\n",
    "\n",
    "\n",
    "        ppl = [exp(i) for i in loss_list]\n",
    "\n",
    "        return ppl[0] if single_input else ppl\n",
    "\n",
    "    def clear_gpu_memory(self) -> None:\n",
    "        \"\"\"Clears GPU memory by deleting references and emptying caches.\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "\n",
    "        # Delete model and tokenizer if they exist\n",
    "        if hasattr(self, 'model'):\n",
    "            del self.model\n",
    "        if hasattr(self, 'tokenizer'):\n",
    "            del self.tokenizer\n",
    "\n",
    "        # Run garbage collection\n",
    "        gc.collect()\n",
    "\n",
    "        # Clear CUDA cache and reset memory stats\n",
    "        with DEVICE:\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "            torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88367884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d81d499c43ed4bdaa046b2b6a0f8dbad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluatr = PerplexityCalculator(\"google/gemma-2-9b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "407745ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score = 2173.306792\n"
     ]
    }
   ],
   "source": [
    "print(f\"score = {sum(evaluatr.get_perplexity(df_sample.text.to_list(), batch_size = 8))/6:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aaa6f2",
   "metadata": {},
   "source": [
    "> 이렇게 계산하는 게 맞나...?(값이 조금 다름)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf50fd0",
   "metadata": {},
   "source": [
    "## 브루트 포스 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a8256fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100) :\n",
    "    evaluatr.get_perplexity(df_sample.text.to_list()[0], batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54ce6436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reindeer mistletoe elf gingerbread family advent'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"reindeer mistletoe elf gingerbread family advent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "061725e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf10a781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reindeer mistletoe elf gingerbread family advent scrooge chimney fireplace ornament\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "model_path = \"google/gemma-2-9b\"\n",
    "words = \"reindeer mistletoe elf gingerbread family advent\"\n",
    "\n",
    "input_texts = [words+\" \"+\" \".join(p) for p in permutations(\"chimney fireplace ornament scrooge\".split())]\n",
    "\n",
    "perplexities = evaluatr.get_perplexity(input_texts, batch_size)\n",
    "best_index = np.argmin(perplexities)\n",
    "print(input_texts[best_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66993844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "467.985588\n"
     ]
    }
   ],
   "source": [
    "print(f\"{min(perplexities):.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
