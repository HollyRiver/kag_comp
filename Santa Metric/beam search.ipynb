{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 샘플의 베스트\n",
    "\n",
    "\"reindeer mistletoe elf gingerbread family advent scrooge chimney fireplace ornament\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc ## for memory\n",
    "import os\n",
    "import copy\n",
    "from math import exp\n",
    "from collections import Counter\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from kaggle_evaluate import PerplexityCalculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2816fc8687b64af6b403beb1d2e94910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluatr = PerplexityCalculator(\"google/gemma-2-9b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_starter = pd.read_csv(\"greedy2.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[467.98558773246197,\n",
       " 840.8178087002444,\n",
       " 967.7753655846766,\n",
       " 762.5886393094895,\n",
       " 302.15420510895507,\n",
       " 120.65904401308495]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluatr.get_perplexity(greedy.text.to_list(), batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` beam size : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = greedy_starter.text.str.split().to_list()\n",
    "beam_size = [5, 10, 10, 15, 25, 30]\n",
    "best_word = []\n",
    "\n",
    "for i, tokens in enumerate(all_tokens) :\n",
    "    token_start = tokens[0]\n",
    "    width = beam_size[i]\n",
    "    token_list = [copy.deepcopy(tokens[1:]) for _ in range(width)]\n",
    "    beam_set = []\n",
    "    \n",
    "    for j in range(len(tokens)-1) :\n",
    "        word_list = []\n",
    "        \n",
    "        if beam_set == [] :            \n",
    "            for t in token_list[0] :\n",
    "                word_list.append(\" \".join([token_start] + [t]))\n",
    "            \n",
    "            perplexities = np.array(evaluatr.get_perplexity(word_list, batch_size = 1024))\n",
    "            min_indexs = perplexities.argsort()[:width]\n",
    "            token_len = len(token_list[0])\n",
    "            beam_set = [[token_start, token_list[min_index//token_len][min_index%token_len]] for min_index in min_indexs]\n",
    "        \n",
    "            for l in range(width) :\n",
    "                del token_list[l][min_indexs[l]%token_len]\n",
    "            \n",
    "        else :\n",
    "            for m, beam in enumerate(beam_set) :\n",
    "                for t in token_list[m] :\n",
    "                    word_list.append(\" \".join(beam + [t]))\n",
    "                \n",
    "            perplexities = np.array(evaluatr.get_perplexity(word_list, batch_size = 1024))\n",
    "            min_indexs = perplexities.argsort()[:width]\n",
    "            token_len = len(token_list[0])\n",
    "            tmp = copy.deepcopy(beam_set)\n",
    "            beam_set = [tmp[min_index//token_len] + [token_list[min_index//token_len][min_index%token_len]] for min_index in min_indexs]\n",
    "        \n",
    "            tmp = copy.deepcopy(token_list)\n",
    "            \n",
    "            for l, min_index in enumerate(min_indexs) :\n",
    "                token_list[l] = copy.deepcopy(tmp[min_index//token_len])\n",
    "                del token_list[l][min_index%token_len]\n",
    "                \n",
    "        \n",
    "    best_word.append(beam_set[perplexities.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame({\"id\":[0,1,2,3,4,5], \"text\":[\" \".join(w) for w in best_word]})\n",
    "df_submission.to_csv(\"greedy2.csv\")\n",
    "\n",
    "os.system(\"kaggle competitions submit -c santa-2024 -f greedy2.csv -m .\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
